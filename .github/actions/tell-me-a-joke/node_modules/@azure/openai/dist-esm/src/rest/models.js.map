{"version":3,"file":"models.js","sourceRoot":"","sources":["../../../src/rest/models.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/**\n * THIS IS AN AUTO-GENERATED FILE - DO NOT EDIT!\n *\n * Any changes you make here may be lost.\n *\n * If you need to make changes, please do so in the original source file, \\{project-root\\}/sources/custom\n */\n\n/**\n * The configuration information for an embeddings request.\n * Embeddings measure the relatedness of text strings and are commonly used for search, clustering,\n * recommendations, and other similar scenarios.\n */\nexport interface EmbeddingsOptions {\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The model name to provide as part of this embeddings request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n  /**\n   * Input texts to get embeddings for, encoded as a an array of strings.\n   * Each input must not exceed 2048 tokens in length.\n   *\n   * Unless you are embedding code, we suggest replacing newlines (\\\\n) in your input with a single space,\n   * as we have observed inferior results when newlines are present.\n   */\n  input: string[];\n}\n\n/**\n * The configuration information for a completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface CompletionsOptions {\n  /** The prompts to generate completions from. */\n  prompt: string[];\n  /** The maximum number of tokens to generate. */\n  max_tokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  top_p?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logit_bias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of completions choices that should be generated per provided prompt as part of an\n   * overall completions response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /**\n   * A value that controls the emission of log probabilities for the provided number of most likely\n   * tokens within a completions response.\n   */\n  logprobs?: number;\n  /**\n   * A value specifying whether completions responses should include input prompts as prefixes to\n   * their generated output.\n   */\n  echo?: boolean;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presence_penalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequency_penalty?: number;\n  /**\n   * A value that controls how many completions will be internally generated prior to response\n   * formulation.\n   * When used together with n, best_of controls the number of candidate completions and must be\n   * greater than n.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  best_of?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\n/**\n * The configuration information for a chat completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface ChatCompletionsOptions {\n  /**\n   * The collection of context messages associated with this chat completions request.\n   * Typical usage begins with a chat message for the System role that provides instructions for\n   * the behavior of the assistant, followed by alternating messages between the User and\n   * Assistant roles.\n   */\n  messages: Array<ChatMessage>;\n  /** A list of functions the model may generate JSON inputs for. */\n  functions?: Array<FunctionDefinition>;\n  /**\n   * Controls how the model responds to function calls. \"none\" means the model does not call a function,\n   * and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.\n   *  Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.\n   *  \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n   */\n  function_call?: string | FunctionName;\n  /** The maximum number of tokens to generate. */\n  max_tokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  top_p?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logit_bias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of chat completions choices that should be generated for a chat completions\n   * response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presence_penalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequency_penalty?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n  /**\n   *   The configuration entries for Azure OpenAI chat extensions that use them.\n   *   This additional specification is only compatible with Azure OpenAI.\n   */\n  dataSources?: Array<AzureChatExtensionConfiguration>;\n}\n\n/** A single, role-attributed message within a chat completion interaction. */\nexport interface ChatMessage {\n  /**\n   * The role associated with this message payload.\n   *\n   * Possible values: system, assistant, user, function, tool\n   */\n  role: string;\n  /** The text associated with this message payload. */\n  content: string | null;\n  /**\n   * The name of the author of this message. `name` is required if role is `function`, and it should be the name of the\n   * function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of\n   * 64 characters.\n   */\n  name?: string;\n  /** The name and arguments of a function that should be called, as generated by the model. */\n  function_call?: FunctionCall;\n  /**\n   *   Additional context data associated with a chat message when requesting chat completions using compatible Azure\n   *   OpenAI chat extensions. This includes information like the intermediate data source retrievals used to service a\n   *   request.\n   *   This context information is only populated when using Azure OpenAI with chat extensions capabilities configured.\n   */\n  context?: AzureChatExtensionsMessageContext;\n}\n\n/** The name and arguments of a function that should be called, as generated by the model. */\nexport interface FunctionCall {\n  /** The name of the function to call. */\n  name: string;\n  /**\n   * The arguments to call the function with, as generated by the model in JSON format.\n   * Note that the model does not always generate valid JSON, and may hallucinate parameters\n   * not defined by your function schema. Validate the arguments in your code before calling\n   * your function.\n   */\n  arguments: string;\n}\n\n/**\n *   A representation of the additional context information available when Azure OpenAI chat extensions are involved\n *   in the generation of a corresponding chat completions response. This context information is only populated when\n *   using an Azure OpenAI request configured to use a matching extension.\n */\nexport interface AzureChatExtensionsMessageContext {\n  /**\n   *   The contextual message payload associated with the Azure chat extensions used for a chat completions request.\n   *   These messages describe the data source retrievals, plugin invocations, and other intermediate steps taken in the\n   *   course of generating a chat completions response that was augmented by capabilities from Azure OpenAI chat\n   *   extensions.\n   */\n  messages?: Array<ChatMessage>;\n}\n\n/** The definition of a caller-specified function that chat completions may invoke in response to matching user input. */\nexport interface FunctionDefinition {\n  /** The name of the function to be called. */\n  name: string;\n  /**\n   * A description of what the function does. The model will use this description when selecting the function and\n   * interpreting its parameters.\n   */\n  description?: string;\n  /** The parameters the functions accepts, described as a JSON Schema object. */\n  parameters?: unknown;\n}\n\n/**\n * A structure that specifies the exact name of a specific, request-provided function to use when processing a chat\n * completions operation.\n */\nexport interface FunctionName {\n  /** The name of the function to call. */\n  name: string;\n}\n\n/**\n *   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\n *   completions request that should use Azure OpenAI chat extensions to augment the response behavior.\n *   The use of this configuration is compatible only with Azure OpenAI.\n */\nexport interface AzureChatExtensionConfiguration {\n  /**\n   *   The label for the type of an Azure chat extension. This typically corresponds to a matching Azure resource.\n   *   Azure chat extensions are only compatible with Azure OpenAI.\n   *\n   * Possible values: AzureCognitiveSearch\n   */\n  type: string;\n  /**\n   *   The configuration payload used for the Azure chat extension. The structure payload details are specific to the\n   *   extension being configured.\n   *   Azure chat extensions are only compatible with Azure OpenAI.\n   */\n  parameters: unknown;\n}\n\n/** Represents the request data used to generate images. */\nexport interface ImageGenerationOptions {\n  /** A description of the desired images. */\n  prompt: string;\n  /** The number of images to generate (defaults to 1). */\n  n?: number;\n  /**\n   * The desired size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 (defaults to 1024x1024).\n   *\n   * Possible values: 256x256, 512x512, 1024x1024\n   */\n  size?: string;\n  /**\n   *   The format in which image generation response items should be presented.\n   *   Azure OpenAI only supports URL response items.\n   *\n   * Possible values: url, b64_json\n   */\n  response_format?: string;\n  /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */\n  user?: string;\n}\n"]}